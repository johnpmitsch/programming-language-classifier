{
 "metadata": {
  "name": "",
  "signature": "sha256:d0079679f80b03465e731a7bdef318e736aa71c8731c21777c1a5815f5a244c8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import numpy as np\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 885
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "import operator\n",
      "import string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 886
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ext_dict = {'clojure': ['.clj', '.clojure'],\n",
      "            'haskell': ['.hs', '.ghc'],\n",
      "            'java': ['.java'],\n",
      "            'javascript': ['.js', '.javascript'],\n",
      "            'ocaml': ['.ml', '.mli', '.ocaml'],\n",
      "            'scheme': ['.scm', '.racket', '.ss'],\n",
      "            'scala': ['.scala'],\n",
      "            'tcl': ['.tcl'],\n",
      "            'perl': ['.perl'],\n",
      "            'php': ['.php'],\n",
      "            'ruby': ['.rb', '.ruby', '.jruby'],\n",
      "            'python': ['.python3', '.python2', '.py']}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 887
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_file(code_example):\n",
      "    \"\"\"Reads file and returns each line of the file as a list of strings\"\"\"\n",
      "    code = open(code_example, \"r\")\n",
      "    try:\n",
      "        return [line.strip('\\n') for line in code.readlines()]\n",
      "    except UnicodeDecodeError or TypeError:\n",
      "        print(\"UnicodeError\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 888
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def type_of_file(ext, ext_dict):\n",
      "    \"\"\"Determines the type of file based on an extension dictionary\"\"\"\n",
      "    for key, value in ext_dict.items():\n",
      "        if ext in value:\n",
      "            return key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 889
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_file_types(root_directory):\n",
      "    \"\"\"Returns the file extensions for each file, as specified by the file dict\"\"\"\n",
      "    file_types = []\n",
      "    for root, directory, files in os.walk(root_directory):\n",
      "        for code_file in files:\n",
      "            file_name, extension = os.path.splitext(code_file)\n",
      "            if extension in [ext for lst in ext_dict.values() for ext in lst]:\n",
      "                snippet = read_file(root + '/' + code_file)\n",
      "                file_types.append(type_of_file(str(extension), ext_dict))\n",
      "    return file_types"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 890
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def n_grams(code_file, length):\n",
      "    \"\"\"Returns x length punctuation characters from a file. Takes a list of strings as input\"\"\"\n",
      "    gram_list = []\n",
      "    for line in code_file:\n",
      "        for i in range(len(line) - length):\n",
      "            gram = line[i:i+length]\n",
      "            grams = list(gram)\n",
      "            punctuation_list = list(string.punctuation)\n",
      "            if set(grams) < set(punctuation_list) :\n",
      "                gram_list.append(gram)\n",
      "    return gram_list    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 891
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def top_values(lst, num):\n",
      "    \"\"\"Returns most reoccuring values for a list of strings\"\"\"\n",
      "    char_list = []\n",
      "    value_dict = collections.defaultdict(lambda: 0) \n",
      "    for value in lst:\n",
      "        value_dict[value] += 1\n",
      "    tup_list = sorted(value_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
      "    for i in range(num):\n",
      "        try:\n",
      "            char_list.append(tup_list[i][0])\n",
      "        except IndexError:\n",
      "            return char_list\n",
      "    return char_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 892
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_common_characters(language, characters, root_dir):\n",
      "    file_types = []\n",
      "    character_list = []\n",
      "    for root, directory, files in os.walk(root_dir):\n",
      "        for code_file in files:\n",
      "            file_name, extension = os.path.splitext(code_file)\n",
      "            if extension in [ext for ext in ext_dict[language]]:\n",
      "                snippet = read_file(root + '/' + code_file)\n",
      "                character_list.extend(n_grams(snippet, characters))\n",
      "    return character_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 893
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_top_value_lists(language, number_of_values):\n",
      "    four = find_common_characters(language, 4, 'bench')\n",
      "    four_list = top_values(four, number_of_values)\n",
      "    three = find_common_characters(language, 3, 'bench')\n",
      "    three_list = top_values(three, number_of_values)\n",
      "    two = find_common_characters(language, 2, 'bench')\n",
      "    two_list = top_values(two, number_of_values)\n",
      "    one = find_common_characters(language, 1, 'bench')\n",
      "    one_list = top_values(one, number_of_values)\n",
      "    return one_list, two_list, three_list, four_list\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 897
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# number_of_values = 2  # default number of top values to return for each language\n",
      "# py1, py2, py3, py4 = get_top_value_lists(\"python\", number_of_values)\n",
      "# cloj1, cloj2, cloj3, cloj4 = get_top_value_lists(\"clojure\", number_of_values)\n",
      "# hask1, hask2, hask3, hask4 = get_top_value_lists(\"haskell\", number_of_values)\n",
      "# java1, java2, java3, java4 = get_top_value_lists(\"java\", number_of_values)\n",
      "# js1, js2, js3, js4 = get_top_value_lists(\"javascript\", number_of_values)\n",
      "#oc1, oc2, oc3, oc4 = get_top_value_lists(\"ocaml\", number_of_values)\n",
      "# scm1, scm2, scm3, scm4 = get_top_value_lists(\"scheme\", number_of_values)\n",
      "# scal1, scal2, scal3, scal4 = get_top_value_lists(\"scala\", number_of_values)\n",
      "# perl1, perl2, perl3, perl4 = get_top_value_lists(\"perl\", number_of_values)\n",
      "# php1, php2, php3, php4 = get_top_value_lists(\"php\", number_of_values)\n",
      "# rb1, rb2, rb3, rb4 = get_top_value_lists(\"ruby\", number_of_values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 898
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# top_characters_per_language = [py1, py2, py3, py4, cloj1, cloj2, cloj3, cloj4, hask1, hask2, hask3, hask4,\n",
      "#                                java1, java2, java3, java4, js1, js2, js3, js4, oc1, oc2, oc3, oc4, scm1,\n",
      "#                                scm2, scm3, scm4, scal1, scal2, scal3, scal4, perl1, perl2, perl3, perl4,\n",
      "#                                php1, php2, php3, php4, rb1, rb2, rb3, rb4] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 899
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top_characters_per_language = [py1, py2, cloj1, cloj2, hask1, hask2, java1, java2, js1, js2, oc1, oc2, scm1, scm2, scal1, scal2, perl1, perl2, php1, php2, rb1, rb2,]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 900
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "one_char_punctuation = list(string.punctuation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 901
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "strings_and_punctuation = ['\"\"\"', '===', '(((', ')))', '/*', '*/', '(*',\n",
      "                           '*)', '{-', '-}', '#|', '|#', '.(', '--', '::', \n",
      "                           '//', '))', '{[', '<-', '<=', '=>', '++', '--', '==',\n",
      "                           'String', 'string', 'class', 'object', 'val', 'def',\n",
      "                           'return', 'set', 'argv', 'end', 'expr', 'public',\n",
      "                           'private', 'function', 'param', 'args', 'format',\n",
      "                           'module', 'let', 'stdlib', 'defn', 'end', 'begin',\n",
      "                           'cut']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 989
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(strings_and_punctuation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 951,
       "text": [
        "24"
       ]
      }
     ],
     "prompt_number": 951
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "strings_and_punctuation.extend(one_char_punctuation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 953
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(strings_and_punctuation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 954,
       "text": [
        "56"
       ]
      }
     ],
     "prompt_number": 954
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def check_snippet(snippet, char_list, length):\n",
      "    count = 0\n",
      "    for line in snippet:\n",
      "        for i in range(len(line) - length):\n",
      "            if line[i:i+length] in char_list:\n",
      "                count += 1\n",
      "    return count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 911
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_features(char_list, number, root_dir):\n",
      "    features = []\n",
      "    for root, directory, files in os.walk(root_dir):\n",
      "        for code_file in files:\n",
      "            if not code_file.startswith('.'):\n",
      "                file_name, extension = os.path.splitext(code_file)\n",
      "                if extension in [ext for lst in ext_dict.values() for ext in lst]:\n",
      "                    snippet = read_file(root + '/' + code_file)\n",
      "                    count = check_snippet(snippet, char_list, number) \n",
      "                    features.append(count)\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 906
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_test_features(char_list, number, root_dir):\n",
      "    features = []\n",
      "    for root, directory, files in os.walk(root_dir):\n",
      "        for code_file in files:\n",
      "            if not code_file.startswith('.'):  # to avoid hidden files like .DS_store\n",
      "                file_name, extension = os.path.splitext(code_file)\n",
      "                snippet = read_file(root + '/' + code_file)\n",
      "                count = check_snippet(snippet, char_list, number)\n",
      "                features.append(count)\n",
      "    return features\n",
      "                "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 907
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_feature_array(char_list, root_dir):\n",
      "    \"\"\"Creates a multi-dimensional array of features for machine learning. \n",
      "    char_list is a list of comparison values, the function iterates through\n",
      "    each list and compares each file in the root directory argument to it\"\"\"\n",
      "    feature_array = []\n",
      "    count = 0\n",
      "    for lst in char_list:\n",
      "        feature = get_features(lst, (count%4 + 1), root_dir)\n",
      "        count +=1\n",
      "        feature_array.append(feature)\n",
      "    return feature_array"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 908
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_test_feature_array(char_list, root_dir):\n",
      "    \"\"\"Creates a multi-dimensional array of features for machine learning. \n",
      "    char_list is a list of comparison values, the function iterates through\n",
      "    each list and compares each file in the root directory argument to it\"\"\"\n",
      "    feature_array = []\n",
      "    count = 0\n",
      "    for lst in char_list:\n",
      "        feature = get_test_features(lst, (count%4 + 1), root_dir)\n",
      "        count +=1\n",
      "        feature_array.append(feature)\n",
      "    return feature_array"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 909
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def check_for_characters(code_file, check_chars):\n",
      "    \"\"\"Checks a file for certain character(s) and returns the count of occurrence in file\"\"\"\n",
      "    snippet = read_file(code_file)\n",
      "    length = len(check_chars)\n",
      "    count = 0\n",
      "    for line in snippet:\n",
      "        for i in range(len(line) - length):\n",
      "            if line[i:i+length] == check_chars:\n",
      "                count += 1\n",
      "    return count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 943
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def check_character_list(code_file, character_list):\n",
      "    feature_count = []\n",
      "    for char in character_list:\n",
      "        count = check_for_characters(code_file, char)\n",
      "        feature_count.append(count)\n",
      "    return feature_count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 945
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_features2(char_list, root_dir):\n",
      "    features = []\n",
      "    for root, directory, files in os.walk(root_dir):\n",
      "        for code_file in files:\n",
      "            if not code_file.startswith('.'):\n",
      "                file_name, extension = os.path.splitext(code_file)\n",
      "                if extension in [ext for lst in ext_dict.values() for ext in lst]:\n",
      "                    the_file = root + '/' + code_file\n",
      "                    count = check_character_list(the_file, char_list) \n",
      "                    features.append(count)\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 956
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_test_features2(char_list, root_dir):\n",
      "    features = {}\n",
      "    for root, directory, files in os.walk(root_dir):\n",
      "        for code_file in files:\n",
      "            if not code_file.startswith('.'):  # to avoid hidden files like .DS_store\n",
      "                the_file = root + '/' + code_file\n",
      "                count = check_character_list(the_file, char_list) \n",
      "                features[int(code_file)] = count  \n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 983
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_features = get_features2(strings_and_punctuation, 'bench')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 957
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_features = get_test_features2(strings_and_punctuation, 'test')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 984
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_feat = []\n",
      "for k, v in test_features.items():\n",
      "    test_feat.append(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 987
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(train_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 958,
       "text": [
        "386"
       ]
      }
     ],
     "prompt_number": 958
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def flip_array(array):\n",
      "    new_array = np.array(array)\n",
      "    new_array = new_array.transpose()\n",
      "    return new_array"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 918
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_feat = flip_array(test_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 971
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_feat = flip_array(train_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 972
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_feat.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 921,
       "text": [
        "(32, 3)"
       ]
      }
     ],
     "prompt_number": 921
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_feat.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 960,
       "text": [
        "(56, 386)"
       ]
      }
     ],
     "prompt_number": 960
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testing = pd.read_csv(\"test.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 856
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testing.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Filename</th>\n",
        "      <th>Language</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> clojure</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2</td>\n",
        "      <td> clojure</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 3</td>\n",
        "      <td> clojure</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 4</td>\n",
        "      <td> clojure</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 5</td>\n",
        "      <td>  python</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 857,
       "text": [
        "   Filename Language\n",
        "0         1  clojure\n",
        "1         2  clojure\n",
        "2         3  clojure\n",
        "3         4  clojure\n",
        "4         5   python"
       ]
      }
     ],
     "prompt_number": 857
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_lang = testing['Language']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 858
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_lang = get_file_types(\"bench\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 859
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(test_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 970,
       "text": [
        "32"
       ]
      }
     ],
     "prompt_number": 970
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn import metrics\n",
      "from sklearn.cross_validation import cross_val_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 860
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_train, x_test, y_train, y_test = train_test_split(train_features, file_types, test_size=0.4, random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 962
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "classifier = RandomForestClassifier()\n",
      "classifier = classifier.fit(x_train, y_train)\n",
      "predicted = classifier.predict(x_test)\n",
      "\n",
      "print(metrics.classification_report(y_test, predicted))\n",
      "print(metrics.confusion_matrix(y_test, predicted))\n",
      "print(metrics.f1_score(y_test, predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "    clojure       0.94      0.94      0.94        16\n",
        "    haskell       0.87      1.00      0.93        13\n",
        "       java       0.64      0.84      0.73        19\n",
        " javascript       0.50      0.17      0.25        12\n",
        "      ocaml       0.80      0.86      0.83        14\n",
        "       perl       1.00      0.94      0.97        17\n",
        "        php       1.00      0.92      0.96        13\n",
        "     python       0.76      1.00      0.87        13\n",
        "       ruby       1.00      0.75      0.86        16\n",
        "      scala       0.87      0.87      0.87        15\n",
        "     scheme       0.88      1.00      0.93         7\n",
        "\n",
        "avg / total       0.84      0.85      0.83       155\n",
        "\n",
        "[[15  0  0  0  0  0  0  0  0  0  1]\n",
        " [ 0 13  0  0  0  0  0  0  0  0  0]\n",
        " [ 0  0 16  2  0  0  0  0  0  1  0]\n",
        " [ 0  0  8  2  1  0  0  0  0  1  0]\n",
        " [ 0  2  0  0 12  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  1 16  0  0  0  0  0]\n",
        " [ 0  0  1  0  0  0 12  0  0  0  0]\n",
        " [ 0  0  0  0  0  0  0 13  0  0  0]\n",
        " [ 0  0  0  0  0  0  0  4 12  0  0]\n",
        " [ 1  0  0  0  1  0  0  0  0 13  0]\n",
        " [ 0  0  0  0  0  0  0  0  0  0  7]]\n",
        "0.831967428937\n"
       ]
      }
     ],
     "prompt_number": 963
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "classifier = LinearSVC()\n",
      "classifier = classifier.fit(x_train, y_train)\n",
      "predicted = classifier.predict(x_test)\n",
      "\n",
      "print(metrics.classification_report(y_test, predicted))\n",
      "print(metrics.confusion_matrix(y_test, predicted))\n",
      "print(metrics.f1_score(y_test, predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "    clojure       1.00      1.00      1.00        16\n",
        "    haskell       1.00      0.92      0.96        13\n",
        "       java       0.74      0.89      0.81        19\n",
        " javascript       0.50      0.08      0.14        12\n",
        "      ocaml       0.93      1.00      0.97        14\n",
        "       perl       1.00      0.76      0.87        17\n",
        "        php       0.67      0.92      0.77        13\n",
        "     python       0.80      0.62      0.70        13\n",
        "       ruby       0.84      1.00      0.91        16\n",
        "      scala       0.72      0.87      0.79        15\n",
        "     scheme       0.78      1.00      0.88         7\n",
        "\n",
        "avg / total       0.83      0.83      0.81       155\n",
        "\n",
        "[[16  0  0  0  0  0  0  0  0  0  0]\n",
        " [ 0 12  0  1  0  0  0  0  0  0  0]\n",
        " [ 0  0 17  0  0  0  0  0  0  2  0]\n",
        " [ 0  0  4  1  0  0  2  1  1  1  2]\n",
        " [ 0  0  0  0 14  0  0  0  0  0  0]\n",
        " [ 0  0  0  0  0 13  4  0  0  0  0]\n",
        " [ 0  0  0  0  1  0 12  0  0  0  0]\n",
        " [ 0  0  1  0  0  0  0  8  2  2  0]\n",
        " [ 0  0  0  0  0  0  0  0 16  0  0]\n",
        " [ 0  0  1  0  0  0  0  1  0 13  0]\n",
        " [ 0  0  0  0  0  0  0  0  0  0  7]]\n",
        "0.809713293191\n"
       ]
      }
     ],
     "prompt_number": 863
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "classifier = LinearSVC()\n",
      "classifier = classifier.fit(train_features, train_lang)\n",
      "predicted = classifier.predict(test_feat)\n",
      "\n",
      "print(metrics.classification_report(test_lang, predicted))\n",
      "print(metrics.confusion_matrix(test_lang, predicted))\n",
      "print(metrics.f1_score(test_lang, predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "    clojure       0.50      0.25      0.33         4\n",
        "    haskell       1.00      0.67      0.80         3\n",
        "       java       0.00      0.00      0.00         2\n",
        " javascript       0.00      0.00      0.00         4\n",
        "      ocaml       0.50      0.50      0.50         2\n",
        "       perl       0.00      0.00      0.00         0\n",
        "        php       0.50      0.33      0.40         3\n",
        "     python       0.38      0.75      0.50         4\n",
        "       ruby       1.00      0.67      0.80         3\n",
        "      scala       0.20      0.50      0.29         2\n",
        "     scheme       0.50      0.33      0.40         3\n",
        "        tcl       0.00      0.00      0.00         2\n",
        "\n",
        "avg / total       0.43      0.38      0.38        32\n",
        "\n",
        "[[1 0 0 1 0 0 0 1 0 0 1 0]\n",
        " [1 2 0 0 0 0 0 0 0 0 0 0]\n",
        " [0 0 0 0 0 0 0 0 0 2 0 0]\n",
        " [0 0 1 0 0 0 1 1 0 1 0 0]\n",
        " [0 0 0 1 1 0 0 0 0 0 0 0]\n",
        " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
        " [0 0 0 0 0 0 1 1 0 1 0 0]\n",
        " [0 0 0 0 1 0 0 3 0 0 0 0]\n",
        " [0 0 0 1 0 0 0 0 2 0 0 0]\n",
        " [0 0 0 0 0 1 0 0 0 1 0 0]\n",
        " [0 0 0 0 0 0 0 2 0 0 1 0]\n",
        " [0 0 0 1 0 1 0 0 0 0 0 0]]\n",
        "0.378273809524\n"
       ]
      }
     ],
     "prompt_number": 986
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "classifier = RandomForestClassifier()\n",
      "classifier = classifier.fit(train_features, train_lang)\n",
      "predicted = classifier.predict(test_feat)\n",
      "\n",
      "print(metrics.classification_report(test_lang, predicted))\n",
      "print(metrics.confusion_matrix(test_lang, predicted))\n",
      "print(metrics.f1_score(test_lang, predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "    clojure       0.17      0.25      0.20         4\n",
        "    haskell       1.00      0.67      0.80         3\n",
        "       java       0.00      0.00      0.00         2\n",
        " javascript       0.00      0.00      0.00         4\n",
        "      ocaml       0.20      0.50      0.29         2\n",
        "       perl       0.00      0.00      0.00         0\n",
        "        php       1.00      0.33      0.50         3\n",
        "     python       0.14      0.25      0.18         4\n",
        "       ruby       0.00      0.00      0.00         3\n",
        "      scala       0.20      0.50      0.29         2\n",
        "     scheme       1.00      0.33      0.50         3\n",
        "        tcl       0.00      0.00      0.00         2\n",
        "\n",
        "avg / total       0.34      0.25      0.25        32\n",
        "\n",
        "[[1 0 0 0 2 0 0 0 0 1 0 0]\n",
        " [1 2 0 0 0 0 0 0 0 0 0 0]\n",
        " [0 0 0 0 0 0 0 0 0 2 0 0]\n",
        " [0 0 0 0 0 0 0 3 0 1 0 0]\n",
        " [0 0 0 1 1 0 0 0 0 0 0 0]\n",
        " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
        " [0 0 0 0 1 0 1 1 0 0 0 0]\n",
        " [1 0 1 0 1 0 0 1 0 0 0 0]\n",
        " [1 0 0 1 0 0 0 1 0 0 0 0]\n",
        " [0 0 0 0 0 1 0 0 0 1 0 0]\n",
        " [0 0 0 1 0 0 0 1 0 0 1 0]\n",
        " [2 0 0 0 0 0 0 0 0 0 0 0]]\n",
        "0.252191558442\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/johnmitsch/.pyenv/versions/sandbox/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n",
        "/Users/johnmitsch/.pyenv/versions/sandbox/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1773: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
        "  'recall', 'true', average, warn_for)\n",
        "/Users/johnmitsch/.pyenv/versions/sandbox/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n",
        "/Users/johnmitsch/.pyenv/versions/sandbox/lib/python3.4/site-packages/sklearn/metrics/metrics.py:1773: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
        "  'recall', 'true', average, warn_for)\n"
       ]
      }
     ],
     "prompt_number": 988
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}